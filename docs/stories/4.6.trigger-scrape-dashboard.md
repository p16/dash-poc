# Story 4.6: Trigger Scrape from Dashboard (Manual/Cron)

## Status
Ready for Development

## Story
**As a** user,
**I want** to see when the last scrape ran and have instructions to manually trigger scraping,
**so that** I can refresh plan data on demand without complex job queue infrastructure.

## Acceptance Criteria

1. Dashboard displays "Last Scrape" timestamp and status
2. "Refresh Data" section shows instructions: "To update plan data, run: npm run scrape:all"
3. Button to copy the command to clipboard for convenience
4. Data freshness indicator (green <24h, yellow 24-48h, red >48h) from Story 4.2
5. After manual scrape completes, user can refresh dashboard to see new data
6. Optional: Link to cron setup documentation for automated scheduling
7. Clean UI with Tailwind CSS styling

**MVP Scope Note:** This story uses manual command execution instead of API-triggered scraping to avoid serverless timeout issues. Scraping runs locally or via cron, not through Next.js API routes.

## Tasks / Subtasks

- [ ] Display last scrape information (AC: 1)
  - [ ] Query: `SELECT MAX(scrape_timestamp) FROM plans`
  - [ ] Display timestamp in user-friendly format
  - [ ] Show status indicator (uses same logic as Story 4.2)
- [ ] Add refresh instructions section (AC: 2)
  - [ ] Create "Refresh Data" card/section
  - [ ] Display instruction text: "To update plan data, run: npm run scrape:all"
  - [ ] Style with Tailwind CSS
- [ ] Add copy-to-clipboard button (AC: 3)
  - [ ] Add button next to command text
  - [ ] Implement clipboard copy functionality
  - [ ] Show success message after copy
- [ ] Implement data freshness indicator (AC: 4)
  - [ ] Reuse freshness logic from Story 4.2
  - [ ] Display green (<24h), yellow (24-48h), red (>48h)
  - [ ] Calculate hours since last scrape
- [ ] Add manual refresh note (AC: 5)
  - [ ] Display message: "After running scrape command, refresh this page to see updated data"
  - [ ] Add manual refresh button (browser refresh)
- [ ] Add cron documentation link (AC: 6)
  - [ ] Create simple cron setup doc (optional)
  - [ ] Example: `0 2 * * * cd /path/to/project && npm run scrape:all`
  - [ ] Link to documentation from dashboard
- [ ] Style UI (AC: 7)
  - [ ] Use Tailwind CSS for clean design
  - [ ] Make instructions prominent but not intrusive
  - [ ] Ensure responsive layout

## Dev Notes

### Prerequisites (Dependencies)
- âœ… Story 2.1: Database Schema (plans table, scrape_timestamp) - **DONE**
- âœ… Story 2.4: Unified Data Collection (npm run scrape command) - **DONE**
- âœ… Story 4.2: Dashboard Home Screen (freshness indicator logic) - **DONE**

### Source Tree
- Dashboard page: `src/app/dashboard/page.tsx` (from Story 4.2 - enhance existing)
- Refresh instructions component: `src/components/dashboard/RefreshInstructions.tsx` (to be created)
- Scraping script: `src/scripts/scrape.ts` (from Story 2.4 - already exists)
- Scrape status utility: `src/lib/dashboard/scrape-status.ts` (from Story 4.2 - reuse)

### Technical Stack
- React Client Component for clipboard functionality
- Clipboard API (`navigator.clipboard.writeText()`)
- Tailwind CSS for styling
- Lucide React icons for UI elements
- Database query for scrape timestamp (reuse Story 4.2 logic)

### MVP Implementation Strategy

**Why Manual Command Execution:**
- Scraping takes 10+ minutes (all 8 collectors)
- Serverless functions have 10-second timeout (Vercel Free tier)
- Serverless functions have 60-second max timeout (Vercel Pro tier)
- API-triggered scraping would timeout before completion
- Job queue infrastructure (BullMQ, Inngest) adds complexity for MVP

**Manual Approach Benefits:**
- Simple and reliable (no timeout issues)
- Easy to set up automated scheduling via cron
- User has full control over scraping schedule
- No additional infrastructure needed

### Implementation Approach

1. **Enhance Dashboard Page** (Story 4.2):
   - Add RefreshInstructions component below ScrapeStatus
   - Reuse existing scrape status query

2. **Build RefreshInstructions Component**:
   - Display "Refresh Data" card/section
   - Show command: `npm run scrape` (or `npm run scrape:all` if aliased)
   - Add copy-to-clipboard button with icon
   - Show success toast/message after copy
   - Include manual refresh note
   - Optional: Link to cron setup docs

3. **Clipboard Functionality**:
   ```typescript
   const copyCommand = async () => {
     await navigator.clipboard.writeText('npm run scrape');
     setShowSuccess(true);
     setTimeout(() => setShowSuccess(false), 2000);
   };
   ```

4. **Data Freshness Indicator**:
   - Reuse `ScrapeStatus` component from Story 4.2
   - Already displays green/yellow/red indicators
   - Already calculates hours since last scrape
   - No changes needed (already implemented)

5. **Cron Setup Documentation** (Optional):
   Create simple markdown doc: `docs/SCRAPING_SETUP.md`
   ```markdown
   ## Automated Scraping with Cron

   To schedule automatic scraping every day at 2 AM:

   1. Edit crontab: `crontab -e`
   2. Add line: `0 2 * * * cd /path/to/project && npm run scrape >> /path/to/logs/scrape.log 2>&1`
   3. Save and exit

   Cron syntax: `minute hour day month weekday command`
   ```

### UI Design

**Refresh Data Card**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”„ Refresh Data                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ To update plan data, run this command:      â”‚
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ npm run scrape     â”‚ [Copy] ğŸ“‹  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                             â”‚
â”‚ â„¹ï¸ After scraping completes, refresh this   â”‚
â”‚   page to see updated data.                â”‚
â”‚                                             â”‚
â”‚ ğŸ“š Automate with cron (documentation)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Testing
- Test file location: `src/components/dashboard/__tests__/RefreshInstructions.test.tsx` (to be created)
- Test framework: Vitest + React Testing Library
- Test scenarios:
  - Component rendering with command text
  - Copy-to-clipboard button functionality
  - Success message display after copy
  - Clipboard API mocking
  - Link to cron documentation (if added)
  - Responsive layout

### Post-MVP Enhancement Path

For future versions, consider:
1. **Job Queue Integration** (BullMQ or Inngest)
   - Create `/api/scrape/trigger` endpoint
   - Queue scraping job with progress tracking
   - Display progress in dashboard
   - Notification when complete

2. **Webhook Integration**
   - Trigger scraping via webhook from external scheduler
   - GitHub Actions or external cron service

3. **Background Jobs**
   - Use Vercel Cron Jobs (Pro tier)
   - Configure scheduled scraping in `vercel.json`

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-16 | 1.2 | Comprehensive Dev Notes: prerequisites, implementation approach, testing strategy | John |
| 2025-01-16 | 1.1 | Status updated to Ready for Development, initial dev notes | John |
| 2025-01-XX | 1.0 | Story extracted from PRD | Architect |

## Dev Agent Record

### Agent Model Used
_To be populated by Dev Agent_

### Debug Log References
_To be populated by Dev Agent_

### Completion Notes List
_To be populated by Dev Agent_

### File List
_To be populated by Dev Agent_

## QA Results
_To be populated by QA Agent_

