# Story 2.4: Unified Data Collection Command & Error Reporting

## Status
Draft

## Story
**As a** developer,
**I want** a single command to execute all data collectors with error reporting,
**so that** the complete collection process can be triggered easily.

## Acceptance Criteria

1. `npm run scrape` command executes all 8 collectors (7 telcos + 1 aggregator) sequentially:
   - Playwright scrapers: O2, Vodafone, Sky, Tesco, Three, Giffgaff
   - API integrations: Smarty, Uswitch
2. Progress logging shows which collector is running and completion status
3. Error summary report generated after all collectors complete (shows successes/failures per source)
4. Collection continues even if individual source fails (fail-safe execution)
5. Final success rate calculated and displayed (must meet 95%+ for acceptance)
6. Execution time logged for performance monitoring (per source and total)
7. Collector results saved to database with timestamps
8. Results also saved to local JSON files for debugging (e.g., `results/o2-2025-11-13T10:30:00.json`)

## Tasks / Subtasks

- [ ] Create unified scraping script (AC: 1)
  - [ ] Create `src/scripts/scrape.ts`
  - [ ] Import all 8 collectors
  - [ ] Implement sequential execution
  - [ ] Test execution flow
- [ ] Implement progress logging (AC: 2)
  - [ ] Log which collector is starting
  - [ ] Log completion status for each collector
  - [ ] Use clear, readable log format
- [ ] Implement error summary report (AC: 3)
  - [ ] Track successes/failures per source
  - [ ] Generate summary report after all collectors complete
  - [ ] Display report in readable format
- [ ] Implement fail-safe execution (AC: 4)
  - [ ] Continue execution if individual collector fails
  - [ ] Catch and log errors per collector
  - [ ] Don't stop entire process on single failure
- [ ] Calculate success rate (AC: 5)
  - [ ] Track total collectors attempted
  - [ ] Track successful collectors
  - [ ] Calculate and display success rate percentage
  - [ ] Verify 95%+ target
- [ ] Log execution time (AC: 6)
  - [ ] Track time per collector
  - [ ] Track total execution time
  - [ ] Display timing information
- [ ] Save results to database (AC: 7)
  - [ ] Ensure all collectors save to database
  - [ ] Verify timestamps are recorded
- [ ] Save results to local files (AC: 8)
  - [ ] Create `results/` directory
  - [ ] Save JSON files with timestamp format
  - [ ] Format: `results/{source}-{timestamp}.json`
- [ ] Create npm script (AC: 1)
  - [ ] Add `scrape` script to `package.json`
  - [ ] Test full execution

## Dev Notes

### Source Tree
- Main scraping script: `src/scripts/scrape.ts`
- Results directory: `results/` (gitignored)
- All collectors: `src/lib/scraping/collectors/`

### Technical Stack
- Sequential execution of all collectors
- Error handling and reporting
- File system operations for result storage
- Database operations for data persistence

### Testing
- Test file location: `tests/integration/scraping/scrape.test.ts` (to be created)
- Test framework: Jest
- Mock all collectors to test orchestration
- Test error handling and reporting
- Test success rate calculation

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-XX | 1.0 | Story extracted from PRD | Architect |

## Dev Agent Record

### Agent Model Used
_To be populated by Dev Agent_

### Debug Log References
_To be populated by Dev Agent_

### Completion Notes List
_To be populated by Dev Agent_

### File List
_To be populated by Dev Agent_

## QA Results
_To be populated by QA Agent_

