# Story 3.2: Prompt Engineering for Competitive Analysis

## Status
Draft

## Story
**As a** developer,
**I want** well-engineered prompts for generating competitive analysis,
**so that** LLM outputs are actionable, strategic, and consistently formatted as JSON.

## Acceptance Criteria

1. **Full Analysis Prompt** (O2 vs All Competitors) created based on proven template with:
   - JSON-only output mandate (strict format, no markdown)
   - Competitiveness scoring model (0-100 scale with weighted factors: Data 40%, Roaming 15%, Extras 15%, Contract Flexibility 10%, Price 20%)
   - Required JSON structure with top-level fields:
     - `analysis_timestamp` (Europe/London timezone)
     - `currency` (GBP)
     - `overall_competitive_sentiments` (5-10 insights with score, sentiment, rationale)
     - `o2_products_analysis` (detailed per-product analysis with comparable plans, sentiments, price suggestions)
     - `full_competitive_dataset_all_plans` (flat dataset of ALL plans analyzed)
     - `products_not_considered` (justification for excluded plans)
   - O2 strategy layer analyzing position across data tiers (Low â‰¤20GB, Medium 21-100GB, Unlimited >100GB)
   - Conversion optimization focus
   - Instructions for handling Uswitch data (brand naming: "o2 uswitch", "vodafone uswitch", etc.)
2. **Custom Comparison Prompt** (Brand A vs Brand B) created as simplified variant:
   - Same JSON structure but focused on two-brand comparison
   - Adaptable brand placeholders (not O2-specific)
   - Maintains scoring model and competitive insights format
3. Prompts include context instructions:
   - Plan data format (pricing, allowances, contract terms, roaming, extras, speed)
   - Source file naming conventions (o2, smarty, vodafone, uswitch, etc.)
   - All contract lengths considered (30-day, 12-month, 24-month)
4. Prompts request specific outputs:
   - Pricing gap identification with specific price recommendations
   - Feature parity analysis (roaming, extras, speed tiers)
   - Strategic recommendations (pricing adjustments, data bundle changes, extras modifications)
   - Competitiveness scores for all plans
5. Prompts optimized for Gemini 2.5 Pro's JSON mode and response format
6. Test with sample plan data validates prompt effectiveness (returns valid JSON, includes all required fields)
7. Prompt templates stored in `/prompts` directory with documentation:
   - `prompt-full-analysis.txt` (O2 vs all competitors)
   - `prompt-custom-comparison.txt` (Brand A vs Brand B)
   - `README.md` explaining scoring model, JSON structure, usage examples
8. Response validation utility handles:
   - JSON parsing errors
   - Missing required fields
   - Invalid data types (e.g., scores not 0-100)
   - Fallback to re-prompt if structure invalid

## Tasks / Subtasks

- [ ] Create prompts directory (AC: 7)
  - [ ] Create `src/lib/llm/prompts/` directory
  - [ ] Set up structure for prompt files
- [ ] Create Full Analysis Prompt (AC: 1)
  - [ ] Create `prompt-full-analysis.txt`
  - [ ] Implement JSON-only output mandate
  - [ ] Define competitiveness scoring model (0-100 scale)
  - [ ] Define required JSON structure with all top-level fields
  - [ ] Add O2 strategy layer for data tiers
  - [ ] Add conversion optimization focus
  - [ ] Add Uswitch data handling instructions
- [ ] Create Custom Comparison Prompt (AC: 2)
  - [ ] Create `prompt-custom-comparison.txt`
  - [ ] Adapt Full Analysis Prompt for two-brand comparison
  - [ ] Use brand placeholders (not O2-specific)
  - [ ] Maintain scoring model and format
- [ ] Add context instructions (AC: 3)
  - [ ] Document plan data format
  - [ ] Document source naming conventions
  - [ ] Document contract length considerations
- [ ] Add output requirements (AC: 4)
  - [ ] Specify pricing gap identification
  - [ ] Specify feature parity analysis
  - [ ] Specify strategic recommendations
  - [ ] Specify competitiveness scores
- [ ] Optimize for Gemini 2.5 Pro (AC: 5)
  - [ ] Test JSON mode compatibility
  - [ ] Optimize prompt structure
  - [ ] Verify response format
- [ ] Test prompts (AC: 6)
  - [ ] Create test with sample plan data
  - [ ] Verify returns valid JSON
  - [ ] Verify includes all required fields
  - [ ] Iterate on prompt if needed
- [ ] Create documentation (AC: 7)
  - [ ] Create `README.md` in prompts directory
  - [ ] Document scoring model
  - [ ] Document JSON structure
  - [ ] Add usage examples
- [ ] Create response validation utility (AC: 8)
  - [ ] Create validation function
  - [ ] Handle JSON parsing errors
  - [ ] Validate required fields
  - [ ] Validate data types (scores 0-100)
  - [ ] Implement fallback to re-prompt if invalid

## Dev Notes

### Source Tree
- Prompt templates: `src/lib/llm/prompts/`
  - `prompt-full-analysis.txt`
  - `prompt-custom-comparison.txt`
  - `README.md`
- Validation utility: `src/lib/llm/validation.ts` (to be created)
- Gemini client: `src/lib/llm/gemini.ts` (from Story 3.1)

### Technical Stack
- Google Gemini 2.5 Pro API with JSON mode
- Prompt engineering for consistent output
- JSON validation and parsing

### Testing
- Test file location: `tests/unit/llm/prompts.test.ts` (to be created)
- Test framework: Jest
- Test prompt templates with sample data
- Test response validation utility
- Manual testing with real API (sparingly)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-XX | 1.0 | Story extracted from PRD | Architect |

## Dev Agent Record

### Agent Model Used
_To be populated by Dev Agent_

### Debug Log References
_To be populated by Dev Agent_

### Completion Notes List
_To be populated by Dev Agent_

### File List
_To be populated by Dev Agent_

## QA Results
_To be populated by QA Agent_

